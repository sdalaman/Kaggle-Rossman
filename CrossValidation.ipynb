{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitors\n",
      "0                2374\n",
      "1                1026\n",
      "2              799542\n",
      "dtype: int64\n",
      "Competitors\n",
      "-1             41255\n",
      " 0              4860\n",
      " 1               864\n",
      " 2               474\n",
      " 3               282\n",
      " 4                96\n",
      " 5                48\n",
      " 6                30\n",
      " 7                18\n",
      " 8                 6\n",
      " 9                 6\n",
      " 12                6\n",
      "dtype: int64\n",
      "The average sale is :6356.40254779\n",
      "We are using the following features: ['Store' 'DayOfWeek' 'Promo' 'Year' 'Competitors']\n",
      "The accuracy of the model is: 86.9\n",
      "Trainingtime is: 124.822999954\n",
      "The RSMEP of the model is: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nahid-Sehir\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1159: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as py\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "# Time variable useful to estimate the time it takes to run this script\n",
    "t0 = time()\n",
    "\n",
    "# Open up the csv files into Pandas dataframes\n",
    "# Opens the train.csv and store.csv and merges them together into data\n",
    "\n",
    "train = pd.DataFrame.from_csv(open('data/train_sub.csv', 'rb',), index_col=None)\n",
    "# Removes all thos rows that have zero sales; this is because zero sales are not scored in the evaluation\n",
    "train = train[train.Sales > 1]\n",
    "\n",
    "\n",
    "extra_data = pd.DataFrame.from_csv(open('data/store.csv', 'rb',), index_col=None)\n",
    "data = pd.merge(train, extra_data, on = 'Store')\n",
    "\n",
    "\n",
    "testdata=pd.DataFrame.from_csv(open('data/test_labeled.csv', 'rb',), index_col=None)\n",
    "testdata = testdata[testdata.Sales > 1]\n",
    "testdata=pd.merge(testdata, extra_data, on = 'Store')\n",
    "\n",
    "testdataLabels=testdata.Sales\n",
    "\n",
    "trainopenzeros=train.Open.tolist()\n",
    "# Only use a fraction of the dat; trick to speed up the calculation; generally not used\n",
    "#data = data[0:len(data)/5]\n",
    "\n",
    "\n",
    "# # Opens the test.csv and store.csv and merges them together into data\n",
    "temp = pd.DataFrame.from_csv('data/test_sub.csv', index_col=None)\n",
    "test = pd.merge(temp, extra_data, on = 'Store')\n",
    "testopenzeros=test.Open.tolist()\n",
    "\n",
    "# Data cleaning, removing NaN from the files\n",
    "# Note that I am not tackling all the variables with NaN here, because there are many I won't use\n",
    "test.loc[(test.Open.isnull()), 'Open'] = 1\n",
    "data.loc[(data.CompetitionDistance.isnull()), 'CompetitionDistance'] = data['CompetitionDistance'].mean()\n",
    "test.loc[(test.CompetitionDistance.isnull()), 'CompetitionDistance'] = data['CompetitionDistance'].mean()\n",
    "\n",
    "# Some variable engineering\n",
    "# These are all commented out because in the end I decided these were not necessary\n",
    "\n",
    "data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "#data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "#data['Day'] = pd.to_datetime(data['Date']).dt.day\n",
    "#data['Date2'] = pd.to_datetime(data['Date']).dt.month\n",
    "#data['StateHoliday2'] = data['StateHoliday'].map( {'a': 1, 'b': 2, 'c': 3, '0': 0, 0:0} ).astype(int)\n",
    "#data['StoreType2'] = data['StoreType'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#data['Assortment2'] = data['Assortment'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#data['PromoInterval'] = data['PromoInterval'].map( {'Feb,May,Aug,Nov': 1, 'Jan,Apr,Jul,Oct': 2, 'Mar,Jun,Sept,Dec': 3, 0:0} ).astype(int)\n",
    "#data['LaborDay'] = np.where( ((data['Month']) == 5) & ((data['Day']) == 1), 1, 0)\n",
    "data['Competitors'] = np.where( (data['DayOfWeek']) == 7, ((data['CompetitionDistance'])/6000), -1)\n",
    "data['Competitors'] = data['Competitors'].astype(int)\n",
    "dictionary = { 0:0, 1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, -1:2}\n",
    "data['Competitors'] = data['Competitors'].map( dictionary )\n",
    "print(data.groupby('Competitors').size())\n",
    "\n",
    "\n",
    "\n",
    "test['Year'] = pd.to_datetime(test['Date']).dt.year\n",
    "#test['Month'] = pd.to_datetime(test['Date']).dt.month\n",
    "#test['Day'] = pd.to_datetime(test['Date']).dt.day\n",
    "#test['Date2'] = pd.to_datetime(test['Date']).dt.month#.dtype='datetime64[ns]\n",
    "#test['StateHoliday2'] = test['StateHoliday'].map( {'a': 1, 'b': 2, 'c': 3, '0': 0, 0:0 } ).astype(int)\n",
    "#test['StoreType2'] = test['StoreType'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#test['Assortment2'] = test['Assortment'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#test['PromoInterval2'] = test['PromoInterval'].map( {'Feb,May,Aug,Nov': 1, 'Jan,Apr,Jul,Oct': 2, 'Mar,Jun,Sept,Dec': 3, 0:0} ).astype(int)\n",
    "#test['Promo'] = 10000*test['Promo']\n",
    "#test['LaborDay'] = np.where( ((test['Month']) == 5) & ((test['Day']) == 1), 1, 0)\n",
    "test['Competitors'] = np.where( (test['DayOfWeek']) == 7, ((test['CompetitionDistance'])/6000), -1)\n",
    "test['Competitors'] = test['Competitors'].astype(int)\n",
    "print(test.groupby('Competitors').size())\n",
    "\n",
    "test['Competitors'] = test['Competitors'].map( dictionary )\n",
    "\n",
    "\n",
    "# Using log(sales) rather than sales. This improves the model precision\n",
    "data['Sales'] = np.log(data['Sales']+1)\n",
    "\n",
    "average = np.exp(data['Sales'].mean())\n",
    "print('The average sale is :' + str(average))\n",
    "\n",
    "# Creates the label array\n",
    "labels_train = data['Sales'].values\n",
    "labels_test=testdata['Sales'].values\n",
    "# Names of features (that were originally in the store, train or test files) we want to drop\n",
    "features_dropped_store = ['StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Promo2']\n",
    "features_dropped_train = ['StateHoliday', 'Open', 'SchoolHoliday', 'Date', 'Customers', 'Sales']\n",
    "features_dropped_test =  ['StateHoliday', 'Open', 'SchoolHoliday', 'Date', 'Id']\n",
    "\n",
    "# Drops all the features in data we do not want to use\n",
    "data = data.drop(features_dropped_train, axis = 1)\n",
    "data = data.drop(features_dropped_store, axis = 1)\n",
    "\n",
    "# Saves the ID column into a series; I will need this when creating the submission file\n",
    "PassID = test['Id']\n",
    "\n",
    "# Drops all the features in test we do not want to use\n",
    "test = test.drop(features_dropped_test, axis=1)\n",
    "test = test.drop(features_dropped_store, axis = 1)\n",
    "\n",
    "print('We are using the following features: ' + str(data.columns.values))\n",
    "\n",
    "# Creates the features array\n",
    "features_train = data.values\n",
    "features_test = test.values\n",
    "\n",
    "# The lines below can be used to select the best features;\n",
    "# however, I prefer to rely on my exploratory analysis, as I fear over-fitting...\n",
    "\n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#selector = SelectKBest(k=3)\n",
    "#selector.fit(features_train, labels_train)\n",
    "#print(selector.scores_)\n",
    "\n",
    "# I have made a function that uses a OneHotEncoder to create dummy variables\n",
    "# for those features that have multiple categorical values\n",
    "# As I explain in the write-up, this is useless for decision tree based regressors\n",
    "\n",
    "def OHE(features):\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder()\n",
    "    features = enc.fit_transform(features)\n",
    "    return features\n",
    "\n",
    "#features_train = OHE(features_train)\n",
    "#features_test  = OHE(features_test)\n",
    "\n",
    "#features_test = enc.transform(features_test)\n",
    "#\n",
    "# MACHINE LEARNING PART\n",
    "#\n",
    "\n",
    "# Imports a bunch of ML regressors and tools\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import grid_search, svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Several ML regressors; RF seem to be the best\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators = 20, n_jobs = -1, min_samples_split = 4)\n",
    "#clf = DecisionTreeRegressor(min_samples_split = 8)\n",
    "#clf = LinearRegression()\n",
    "#clf = AdaBoostRegressor(n_estimators = 100)\n",
    "#clf = GradientBoostingRegressor(n_estimators = 100)\n",
    "#clf = BaggingRegressor()\n",
    "\n",
    "# Parameters for grid_search;\n",
    "# RF needs very little parameter tweaking, so I commented these out\n",
    "\n",
    "#parameters = {'max_features':['auto', 'sqrt', 'log2']}\n",
    "#svr = RandomForestRegressor(n_jobs = 1, n_estimators = 1)\n",
    "#clf = grid_search.GridSearchCV(svr, parameters)\n",
    "#print clf.best_estimator_\n",
    "\n",
    "\n",
    "# Fits the regressor, makes a prediction and then calculates various metrics\n",
    "zeros=[]\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_train)\n",
    "print('The accuracy of the model is: ' + str(round(100*(r2_score(labels_train, pred)),2)))\n",
    "print('Trainingtime is: ' + str(time() - t0))\n",
    "\n",
    "# This function calculates the Root Mean Square Percentage Error\n",
    "def RMSEP(pred, labels):\n",
    "    rmsep = 0\n",
    "    for i in range(0,len(pred)):\n",
    "        rmsep += ((np.exp(labels[i])-np.exp(pred[i]))/np.exp(labels[i]))**2\n",
    "    rmsep = rmsep / len(pred)\n",
    "    rmsep = math.sqrt(rmsep)\n",
    "    return rmsep\n",
    "\n",
    "# This is the same metric as used on the Kaggle leaderboard\n",
    "print('The RSMEP of the model is: ' + str(round(RMSEP(pred, labels_train),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sale during the test period: 7124.14214273\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(features_test)\n",
    "pred = np.exp(pred) - 1\n",
    "pred = pred\n",
    "print('Average sale during the test period: ' + str(pred.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47945"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41396"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdataLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Sales'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e97fdafb3962>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test_sub.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtestdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSales\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Store'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Nahid-Sehir\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1945\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1946\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n\u001b[1;32m-> 1947\u001b[1;33m                                  (type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Sales'"
     ]
    }
   ],
   "source": [
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as py\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "# Time variable useful to estimate the time it takes to run this script\n",
    "t0 = time()\n",
    "\n",
    "# Open up the csv files into Pandas dataframes\n",
    "# Opens the train.csv and store.csv and merges them together into data\n",
    "\n",
    "train = pd.DataFrame.from_csv(open('data/train_sub.csv', 'rb',), index_col=None)\n",
    "# Removes all thos rows that have zero sales; this is because zero sales are not scored in the evaluation\n",
    "train = train[train.Sales > 1]\n",
    "\n",
    "\n",
    "extra_data = pd.DataFrame.from_csv(open('data/store.csv', 'rb',), index_col=None)\n",
    "data = pd.merge(train, extra_data, on = 'Store')\n",
    "\n",
    "\n",
    "testdata=pd.DataFrame.from_csv(open('data/test_sub.csv', 'rb',), index_col=None)\n",
    "testdata = testdata[testdata.Sales > 1]\n",
    "testdata=pd.merge(testdata, extra_data, on = 'Store')\n",
    "\n",
    "testdataLabels=pd.DataFrame.from_csv(open('data/test_labeled.csv', 'rb',), index_col=None)\n",
    "testdataLabels=testdataLabels.Sales\n",
    "\n",
    "trainopenzeros=train.Open.tolist()\n",
    "# Only use a fraction of the dat; trick to speed up the calculation; generally not used\n",
    "#data = data[0:len(data)/5]\n",
    "\n",
    "\n",
    "# # Opens the test.csv and store.csv and merges them together into data\n",
    "temp = pd.DataFrame.from_csv('data/test_sub.csv', index_col=None)\n",
    "test = pd.merge(temp, extra_data, on = 'Store')\n",
    "testopenzeros=test.Open.tolist()\n",
    "\n",
    "# Data cleaning, removing NaN from the files\n",
    "# Note that I am not tackling all the variables with NaN here, because there are many I won't use\n",
    "test.loc[(test.Open.isnull()), 'Open'] = 1\n",
    "data.loc[(data.CompetitionDistance.isnull()), 'CompetitionDistance'] = data['CompetitionDistance'].mean()\n",
    "test.loc[(test.CompetitionDistance.isnull()), 'CompetitionDistance'] = data['CompetitionDistance'].mean()\n",
    "\n",
    "# Some variable engineering\n",
    "# These are all commented out because in the end I decided these were not necessary\n",
    "\n",
    "data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "#data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "#data['Day'] = pd.to_datetime(data['Date']).dt.day\n",
    "#data['Date2'] = pd.to_datetime(data['Date']).dt.month\n",
    "#data['StateHoliday2'] = data['StateHoliday'].map( {'a': 1, 'b': 2, 'c': 3, '0': 0, 0:0} ).astype(int)\n",
    "#data['StoreType2'] = data['StoreType'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#data['Assortment2'] = data['Assortment'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#data['PromoInterval'] = data['PromoInterval'].map( {'Feb,May,Aug,Nov': 1, 'Jan,Apr,Jul,Oct': 2, 'Mar,Jun,Sept,Dec': 3, 0:0} ).astype(int)\n",
    "#data['LaborDay'] = np.where( ((data['Month']) == 5) & ((data['Day']) == 1), 1, 0)\n",
    "data['Competitors'] = np.where( (data['DayOfWeek']) == 7, ((data['CompetitionDistance'])/6000), -1)\n",
    "data['Competitors'] = data['Competitors'].astype(int)\n",
    "dictionary = { 0:0, 1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, -1:2}\n",
    "data['Competitors'] = data['Competitors'].map( dictionary )\n",
    "print(data.groupby('Competitors').size())\n",
    "\n",
    "\n",
    "\n",
    "test['Year'] = pd.to_datetime(test['Date']).dt.year\n",
    "#test['Month'] = pd.to_datetime(test['Date']).dt.month\n",
    "#test['Day'] = pd.to_datetime(test['Date']).dt.day\n",
    "#test['Date2'] = pd.to_datetime(test['Date']).dt.month#.dtype='datetime64[ns]\n",
    "#test['StateHoliday2'] = test['StateHoliday'].map( {'a': 1, 'b': 2, 'c': 3, '0': 0, 0:0 } ).astype(int)\n",
    "#test['StoreType2'] = test['StoreType'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#test['Assortment2'] = test['Assortment'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#test['PromoInterval2'] = test['PromoInterval'].map( {'Feb,May,Aug,Nov': 1, 'Jan,Apr,Jul,Oct': 2, 'Mar,Jun,Sept,Dec': 3, 0:0} ).astype(int)\n",
    "#test['Promo'] = 10000*test['Promo']\n",
    "#test['LaborDay'] = np.where( ((test['Month']) == 5) & ((test['Day']) == 1), 1, 0)\n",
    "test['Competitors'] = np.where( (test['DayOfWeek']) == 7, ((test['CompetitionDistance'])/6000), -1)\n",
    "test['Competitors'] = test['Competitors'].astype(int)\n",
    "print(test.groupby('Competitors').size())\n",
    "\n",
    "test['Competitors'] = test['Competitors'].map( dictionary )\n",
    "\n",
    "\n",
    "# Using log(sales) rather than sales. This improves the model precision\n",
    "data['Sales'] = np.log(data['Sales']+1)\n",
    "\n",
    "average = np.exp(data['Sales'].mean())\n",
    "print('The average sale is :' + str(average))\n",
    "\n",
    "# Creates the label array\n",
    "labels_train = data['Sales'].values\n",
    "labels_test=testdata['Sales'].values\n",
    "# Names of features (that were originally in the store, train or test files) we want to drop\n",
    "features_dropped_store = ['StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Promo2']\n",
    "features_dropped_train = ['StateHoliday', 'Open', 'SchoolHoliday', 'Date', 'Customers', 'Sales']\n",
    "features_dropped_test =  ['StateHoliday', 'Open', 'SchoolHoliday', 'Date', 'Id']\n",
    "\n",
    "# Drops all the features in data we do not want to use\n",
    "data = data.drop(features_dropped_train, axis = 1)\n",
    "data = data.drop(features_dropped_store, axis = 1)\n",
    "\n",
    "# Saves the ID column into a series; I will need this when creating the submission file\n",
    "PassID = test['Id']\n",
    "\n",
    "# Drops all the features in test we do not want to use\n",
    "test = test.drop(features_dropped_test, axis=1)\n",
    "test = test.drop(features_dropped_store, axis = 1)\n",
    "\n",
    "print('We are using the following features: ' + str(data.columns.values))\n",
    "\n",
    "# Creates the features array\n",
    "features_train = data.values\n",
    "features_test = test.values\n",
    "\n",
    "# The lines below can be used to select the best features;\n",
    "# however, I prefer to rely on my exploratory analysis, as I fear over-fitting...\n",
    "\n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#selector = SelectKBest(k=3)\n",
    "#selector.fit(features_train, labels_train)\n",
    "#print(selector.scores_)\n",
    "\n",
    "# I have made a function that uses a OneHotEncoder to create dummy variables\n",
    "# for those features that have multiple categorical values\n",
    "# As I explain in the write-up, this is useless for decision tree based regressors\n",
    "\n",
    "def OHE(features):\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder()\n",
    "    features = enc.fit_transform(features)\n",
    "    return features\n",
    "\n",
    "#features_train = OHE(features_train)\n",
    "#features_test  = OHE(features_test)\n",
    "\n",
    "#features_test = enc.transform(features_test)\n",
    "#\n",
    "# MACHINE LEARNING PART\n",
    "#\n",
    "\n",
    "# Imports a bunch of ML regressors and tools\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import grid_search, svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Several ML regressors; RF seem to be the best\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators = 20, n_jobs = -1, min_samples_split = 4)\n",
    "#clf = DecisionTreeRegressor(min_samples_split = 8)\n",
    "#clf = LinearRegression()\n",
    "#clf = AdaBoostRegressor(n_estimators = 100)\n",
    "#clf = GradientBoostingRegressor(n_estimators = 100)\n",
    "#clf = BaggingRegressor()\n",
    "\n",
    "# Parameters for grid_search;\n",
    "# RF needs very little parameter tweaking, so I commented these out\n",
    "\n",
    "#parameters = {'max_features':['auto', 'sqrt', 'log2']}\n",
    "#svr = RandomForestRegressor(n_jobs = 1, n_estimators = 1)\n",
    "#clf = grid_search.GridSearchCV(svr, parameters)\n",
    "#print clf.best_estimator_\n",
    "\n",
    "\n",
    "# Fits the regressor, makes a prediction and then calculates various metrics\n",
    "zeros=[]\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_train)\n",
    "print('The accuracy of the model is: ' + str(round(100*(r2_score(labels_train, pred)),2)))\n",
    "print('Trainingtime is: ' + str(time() - t0))\n",
    "\n",
    "# This function calculates the Root Mean Square Percentage Error\n",
    "def RMSEP(pred, labels):\n",
    "    rmsep = 0\n",
    "    for i in range(0,len(pred)):\n",
    "        rmsep += ((np.exp(labels[i])-np.exp(pred[i]))/np.exp(labels[i]))**2\n",
    "    rmsep = rmsep / len(pred)\n",
    "    rmsep = math.sqrt(rmsep)\n",
    "    return rmsep\n",
    "\n",
    "# This is the same metric as used on the Kaggle leaderboard\n",
    "print('The RSMEP of the model is: ' + str(round(RMSEP(pred, labels_train),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitors\n",
      "0                2374\n",
      "1                1026\n",
      "2              799542\n",
      "dtype: int64\n",
      "Competitors\n",
      "-1             41255\n",
      " 0              4860\n",
      " 1               864\n",
      " 2               474\n",
      " 3               282\n",
      " 4                96\n",
      " 5                48\n",
      " 6                30\n",
      " 7                18\n",
      " 8                 6\n",
      " 9                 6\n",
      " 12                6\n",
      "dtype: int64\n",
      "The average sale is :6356.40254779\n",
      "We are using the following features: ['Store' 'DayOfWeek' 'Promo' 'Year' 'Competitors']\n",
      "The accuracy of the model is: 86.9\n",
      "Trainingtime is: 142.861999989\n"
     ]
    }
   ],
   "source": [
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as py\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "# Time variable useful to estimate the time it takes to run this script\n",
    "t0 = time()\n",
    "\n",
    "# Open up the csv files into Pandas dataframes\n",
    "# Opens the train.csv and store.csv and merges them together into data\n",
    "\n",
    "train = pd.DataFrame.from_csv(open('data/train_sub.csv', 'rb',), index_col=None)\n",
    "# Removes all thos rows that have zero sales; this is because zero sales are not scored in the evaluation\n",
    "train = train[train.Sales > 1]\n",
    "\n",
    "\n",
    "extra_data = pd.DataFrame.from_csv(open('data/store.csv', 'rb',), index_col=None)\n",
    "data = pd.merge(train, extra_data, on = 'Store')\n",
    "\n",
    "\n",
    "testdata=pd.DataFrame.from_csv(open('data/test_sub.csv', 'rb',), index_col=None)\n",
    "testdata=pd.merge(testdata, extra_data, on = 'Store')\n",
    "\n",
    "testdataLabels=pd.DataFrame.from_csv(open('data/test_labeled.csv', 'rb',), index_col=None)\n",
    "testdataLabels=testdataLabels.Sales\n",
    "\n",
    "trainopenzeros=train.Open.tolist()\n",
    "# Only use a fraction of the dat; trick to speed up the calculation; generally not used\n",
    "#data = data[0:len(data)/5]\n",
    "\n",
    "\n",
    "# # Opens the test.csv and store.csv and merges them together into data\n",
    "temp = pd.DataFrame.from_csv('data/test_sub.csv', index_col=None)\n",
    "test = pd.merge(temp, extra_data, on = 'Store')\n",
    "testopenzeros=test.Open.tolist()\n",
    "\n",
    "# Data cleaning, removing NaN from the files\n",
    "# Note that I am not tackling all the variables with NaN here, because there are many I won't use\n",
    "test.loc[(test.Open.isnull()), 'Open'] = 1\n",
    "data.loc[(data.CompetitionDistance.isnull()), 'CompetitionDistance'] = data['CompetitionDistance'].mean()\n",
    "test.loc[(test.CompetitionDistance.isnull()), 'CompetitionDistance'] = data['CompetitionDistance'].mean()\n",
    "\n",
    "# Some variable engineering\n",
    "# These are all commented out because in the end I decided these were not necessary\n",
    "\n",
    "data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "#data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "#data['Day'] = pd.to_datetime(data['Date']).dt.day\n",
    "#data['Date2'] = pd.to_datetime(data['Date']).dt.month\n",
    "#data['StateHoliday2'] = data['StateHoliday'].map( {'a': 1, 'b': 2, 'c': 3, '0': 0, 0:0} ).astype(int)\n",
    "#data['StoreType2'] = data['StoreType'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#data['Assortment2'] = data['Assortment'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#data['PromoInterval'] = data['PromoInterval'].map( {'Feb,May,Aug,Nov': 1, 'Jan,Apr,Jul,Oct': 2, 'Mar,Jun,Sept,Dec': 3, 0:0} ).astype(int)\n",
    "#data['LaborDay'] = np.where( ((data['Month']) == 5) & ((data['Day']) == 1), 1, 0)\n",
    "data['Competitors'] = np.where( (data['DayOfWeek']) == 7, ((data['CompetitionDistance'])/6000), -1)\n",
    "data['Competitors'] = data['Competitors'].astype(int)\n",
    "dictionary = { 0:0, 1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, -1:2}\n",
    "data['Competitors'] = data['Competitors'].map( dictionary )\n",
    "print(data.groupby('Competitors').size())\n",
    "\n",
    "\n",
    "\n",
    "test['Year'] = pd.to_datetime(test['Date']).dt.year\n",
    "#test['Month'] = pd.to_datetime(test['Date']).dt.month\n",
    "#test['Day'] = pd.to_datetime(test['Date']).dt.day\n",
    "#test['Date2'] = pd.to_datetime(test['Date']).dt.month#.dtype='datetime64[ns]\n",
    "#test['StateHoliday2'] = test['StateHoliday'].map( {'a': 1, 'b': 2, 'c': 3, '0': 0, 0:0 } ).astype(int)\n",
    "#test['StoreType2'] = test['StoreType'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#test['Assortment2'] = test['Assortment'].map( {'a': 1, 'b': 2, 'c': 3, 'd': 4} ).astype(int)\n",
    "#test['PromoInterval2'] = test['PromoInterval'].map( {'Feb,May,Aug,Nov': 1, 'Jan,Apr,Jul,Oct': 2, 'Mar,Jun,Sept,Dec': 3, 0:0} ).astype(int)\n",
    "#test['Promo'] = 10000*test['Promo']\n",
    "#test['LaborDay'] = np.where( ((test['Month']) == 5) & ((test['Day']) == 1), 1, 0)\n",
    "test['Competitors'] = np.where( (test['DayOfWeek']) == 7, ((test['CompetitionDistance'])/6000), -1)\n",
    "test['Competitors'] = test['Competitors'].astype(int)\n",
    "print(test.groupby('Competitors').size())\n",
    "\n",
    "test['Competitors'] = test['Competitors'].map( dictionary )\n",
    "\n",
    "\n",
    "# Using log(sales) rather than sales. This improves the model precision\n",
    "data['Sales'] = np.log(data['Sales']+1)\n",
    "\n",
    "average = np.exp(data['Sales'].mean())\n",
    "print('The average sale is :' + str(average))\n",
    "\n",
    "# Creates the label array\n",
    "labels_train = data['Sales'].values\n",
    "# Names of features (that were originally in the store, train or test files) we want to drop\n",
    "features_dropped_store = ['StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'Promo2']\n",
    "features_dropped_train = ['StateHoliday', 'Open', 'SchoolHoliday', 'Date', 'Customers', 'Sales']\n",
    "features_dropped_test =  ['StateHoliday', 'Open', 'SchoolHoliday', 'Date', 'Id']\n",
    "\n",
    "# Drops all the features in data we do not want to use\n",
    "data = data.drop(features_dropped_train, axis = 1)\n",
    "data = data.drop(features_dropped_store, axis = 1)\n",
    "\n",
    "# Saves the ID column into a series; I will need this when creating the submission file\n",
    "PassID = test['Id']\n",
    "\n",
    "# Drops all the features in test we do not want to use\n",
    "test = test.drop(features_dropped_test, axis=1)\n",
    "test = test.drop(features_dropped_store, axis = 1)\n",
    "\n",
    "print('We are using the following features: ' + str(data.columns.values))\n",
    "\n",
    "# Creates the features array\n",
    "features_train = data.values\n",
    "features_test = test.values\n",
    "\n",
    "# The lines below can be used to select the best features;\n",
    "# however, I prefer to rely on my exploratory analysis, as I fear over-fitting...\n",
    "\n",
    "#from sklearn.feature_selection import SelectKBest\n",
    "#selector = SelectKBest(k=3)\n",
    "#selector.fit(features_train, labels_train)\n",
    "#print(selector.scores_)\n",
    "\n",
    "# I have made a function that uses a OneHotEncoder to create dummy variables\n",
    "# for those features that have multiple categorical values\n",
    "# As I explain in the write-up, this is useless for decision tree based regressors\n",
    "\n",
    "def OHE(features):\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder()\n",
    "    features = enc.fit_transform(features)\n",
    "    return features\n",
    "\n",
    "#features_train = OHE(features_train)\n",
    "#features_test  = OHE(features_test)\n",
    "\n",
    "#features_test = enc.transform(features_test)\n",
    "#\n",
    "# MACHINE LEARNING PART\n",
    "#\n",
    "\n",
    "# Imports a bunch of ML regressors and tools\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import grid_search, svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Several ML regressors; RF seem to be the best\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators = 20, n_jobs = -1, min_samples_split = 4)\n",
    "#clf = DecisionTreeRegressor(min_samples_split = 8)\n",
    "#clf = LinearRegression()\n",
    "#clf = AdaBoostRegressor(n_estimators = 100)\n",
    "#clf = GradientBoostingRegressor(n_estimators = 100)\n",
    "#clf = BaggingRegressor()\n",
    "\n",
    "# Parameters for grid_search;\n",
    "# RF needs very little parameter tweaking, so I commented these out\n",
    "\n",
    "#parameters = {'max_features':['auto', 'sqrt', 'log2']}\n",
    "#svr = RandomForestRegressor(n_jobs = 1, n_estimators = 1)\n",
    "#clf = grid_search.GridSearchCV(svr, parameters)\n",
    "#print clf.best_estimator_\n",
    "\n",
    "\n",
    "# Fits the regressor, makes a prediction and then calculates various metrics\n",
    "zeros=[]\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_train)\n",
    "print('The accuracy of the model is: ' + str(round(100*(r2_score(labels_train, pred)),2)))\n",
    "print('Trainingtime is: ' + str(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
