{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe\n",
    "\n",
    "def toBinary(featureCol, df):\n",
    "    values = set(df[featureCol].unique())\n",
    "    newCol = [featureCol + '_' + val for val in values]\n",
    "    for val in values:\n",
    "        df[featureCol + '_' + val] = df[featureCol].map(lambda x: 1 if x == val else 0)\n",
    "    return newCol\n",
    "\n",
    "# Gather some features\n",
    "def build_features(features, data):\n",
    "    # remove NaNs\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                     'CompetitionOpenSinceYear', 'Promo', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear'])\n",
    "\n",
    "    # add some more with a bit of preprocessing\n",
    "    features.append('SchoolHoliday')\n",
    "    data['SchoolHoliday'] = data['SchoolHoliday'].astype(float)\n",
    "    #\n",
    "    #features.append('StateHoliday')\n",
    "    #data.loc[data['StateHoliday'] == 'a', 'StateHoliday'] = '1'\n",
    "    #data.loc[data['StateHoliday'] == 'b', 'StateHoliday'] = '2'\n",
    "    #data.loc[data['StateHoliday'] == 'c', 'StateHoliday'] = '3'\n",
    "    #data['StateHoliday'] = data['StateHoliday'].astype(float)\n",
    "\n",
    "    features.append('DayOfWeek')\n",
    "    features.append('month')\n",
    "    features.append('day')\n",
    "    features.append('year')\n",
    "    data['year'] = data.Date.apply(lambda x: x.split('-')[0])\n",
    "    data['year'] = data['year'].astype(float)\n",
    "    data['month'] = data.Date.apply(lambda x: x.split('-')[1])\n",
    "    data['month'] = data['month'].astype(float)\n",
    "    data['day'] = data.Date.apply(lambda x: x.split('-')[2])\n",
    "    data['day'] = data['day'].astype(float)\n",
    "\n",
    "    # features.append('StoreType')\n",
    "    for x in ['a', 'b', 'c', 'd']:\n",
    "        features.append('StoreType_' + x)\n",
    "        data['StoreType_' + x] = data['StoreType'].map(lambda y: 1 if y == x else 0)\n",
    "        \n",
    "    # data.loc[data['StoreType'] == 'a', 'StoreType'] = '1'\n",
    "    # data.loc[data['StoreType'] == 'b', 'StoreType'] = '2'\n",
    "    # data.loc[data['StoreType'] == 'c', 'StoreType'] = '3'\n",
    "    # data.loc[data['StoreType'] == 'd', 'StoreType'] = '4'\n",
    "    # data['StoreType'] = data['StoreType'].astype(float)\n",
    "\n",
    "    newCol = toBinary('Assortment', data)\n",
    "    features += newCol\n",
    "    # features.append('Assortment')\n",
    "    # data.loc[data['Assortment'] == 'a', 'Assortment'] = '1'\n",
    "    # data.loc[data['Assortment'] == 'b', 'Assortment'] = '2'\n",
    "    # data.loc[data['Assortment'] == 'c', 'Assortment'] = '3'\n",
    "    # data['Assortment'] = data['Assortment'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training, test and store data using pandas\n"
     ]
    }
   ],
   "source": [
    "print(\"Load the training, test and store data using pandas\")\n",
    "train = pd.read_csv(\"train.csv\",dtype={'StateHoliday': str})\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "store = pd.read_csv(\"store.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume store open, if not provided\n"
     ]
    }
   ],
   "source": [
    "print(\"Assume store open, if not provided\")\n",
    "test.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider only open stores for training. Closed stores wont count into the score.\n"
     ]
    }
   ],
   "source": [
    "print(\"Consider only open stores for training. Closed stores wont count into the score.\")\n",
    "train = train[train[\"Open\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join with store\n"
     ]
    }
   ],
   "source": [
    "print(\"Join with store\")\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment features\n",
      "['Store', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'SchoolHoliday', 'DayOfWeek', 'month', 'day', 'year', 'StoreType_a', 'StoreType_b', 'StoreType_c', 'StoreType_d', 'Assortment_b', 'Assortment_a', 'Assortment_c']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "print(\"augment features\")\n",
    "build_features(features, train)\n",
    "build_features([], test)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"eta\": 0.3,\n",
    "          \"max_depth\": 8,\n",
    "          \"subsample\": 0.7,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1\n",
    "          }\n",
    "num_trees = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until train error hasn't decreased in 50 rounds.\n",
      "[0]\teval-rmspe:0.996821\ttrain-rmspe:0.996811\n",
      "[1]\teval-rmspe:0.981414\ttrain-rmspe:0.981505\n",
      "[2]\teval-rmspe:0.937583\ttrain-rmspe:0.937983\n",
      "[3]\teval-rmspe:0.855412\ttrain-rmspe:0.856349\n",
      "[4]\teval-rmspe:0.742078\ttrain-rmspe:0.743698\n",
      "[5]\teval-rmspe:0.616666\ttrain-rmspe:0.619244\n",
      "[6]\teval-rmspe:0.501116\ttrain-rmspe:0.504874\n",
      "[7]\teval-rmspe:0.411137\ttrain-rmspe:0.415988\n",
      "[8]\teval-rmspe:0.349545\ttrain-rmspe:0.356177\n",
      "[9]\teval-rmspe:0.318484\ttrain-rmspe:0.325083\n",
      "[10]\teval-rmspe:0.307534\ttrain-rmspe:0.314209\n",
      "[11]\teval-rmspe:0.291402\ttrain-rmspe:0.301123\n",
      "[12]\teval-rmspe:0.287980\ttrain-rmspe:0.298636\n",
      "[13]\teval-rmspe:0.285262\ttrain-rmspe:0.294740\n",
      "[14]\teval-rmspe:0.287124\ttrain-rmspe:0.296820\n",
      "[15]\teval-rmspe:0.287741\ttrain-rmspe:0.297490\n",
      "[16]\teval-rmspe:0.287304\ttrain-rmspe:0.296737\n",
      "[17]\teval-rmspe:0.286897\ttrain-rmspe:0.296737\n",
      "[18]\teval-rmspe:0.286049\ttrain-rmspe:0.294298\n",
      "[19]\teval-rmspe:0.285580\ttrain-rmspe:0.294367\n",
      "[20]\teval-rmspe:0.284825\ttrain-rmspe:0.292823\n",
      "[21]\teval-rmspe:0.283084\ttrain-rmspe:0.291086\n",
      "[22]\teval-rmspe:0.275409\ttrain-rmspe:0.283674\n",
      "[23]\teval-rmspe:0.275042\ttrain-rmspe:0.283550\n",
      "[24]\teval-rmspe:0.270874\ttrain-rmspe:0.279441\n",
      "[25]\teval-rmspe:0.263872\ttrain-rmspe:0.273827\n",
      "[26]\teval-rmspe:0.253126\ttrain-rmspe:0.266339\n",
      "[27]\teval-rmspe:0.251389\ttrain-rmspe:0.264324\n",
      "[28]\teval-rmspe:0.250303\ttrain-rmspe:0.263216\n",
      "[29]\teval-rmspe:0.248297\ttrain-rmspe:0.261025\n",
      "[30]\teval-rmspe:0.242713\ttrain-rmspe:0.256734\n",
      "[31]\teval-rmspe:0.235571\ttrain-rmspe:0.251279\n",
      "[32]\teval-rmspe:0.236394\ttrain-rmspe:0.251002\n",
      "[33]\teval-rmspe:0.235016\ttrain-rmspe:0.249328\n",
      "[34]\teval-rmspe:0.232188\ttrain-rmspe:0.246446\n",
      "[35]\teval-rmspe:0.229718\ttrain-rmspe:0.243907\n",
      "[36]\teval-rmspe:0.226771\ttrain-rmspe:0.241194\n",
      "[37]\teval-rmspe:0.223585\ttrain-rmspe:0.237091\n",
      "[38]\teval-rmspe:0.218416\ttrain-rmspe:0.232552\n",
      "[39]\teval-rmspe:0.214907\ttrain-rmspe:0.220420\n",
      "[40]\teval-rmspe:0.214338\ttrain-rmspe:0.219833\n",
      "[41]\teval-rmspe:0.214732\ttrain-rmspe:0.219520\n",
      "[42]\teval-rmspe:0.213673\ttrain-rmspe:0.217991\n",
      "[43]\teval-rmspe:0.211358\ttrain-rmspe:0.215496\n",
      "[44]\teval-rmspe:0.209465\ttrain-rmspe:0.213948\n",
      "[45]\teval-rmspe:0.205494\ttrain-rmspe:0.211372\n",
      "[46]\teval-rmspe:0.204499\ttrain-rmspe:0.210221\n",
      "[47]\teval-rmspe:0.202242\ttrain-rmspe:0.208260\n",
      "[48]\teval-rmspe:0.201358\ttrain-rmspe:0.207348\n",
      "[49]\teval-rmspe:0.200418\ttrain-rmspe:0.206596\n",
      "[50]\teval-rmspe:0.199360\ttrain-rmspe:0.205404\n",
      "[51]\teval-rmspe:0.197962\ttrain-rmspe:0.204342\n",
      "[52]\teval-rmspe:0.196401\ttrain-rmspe:0.203212\n",
      "[53]\teval-rmspe:0.194813\ttrain-rmspe:0.201852\n",
      "[54]\teval-rmspe:0.193439\ttrain-rmspe:0.200249\n",
      "[55]\teval-rmspe:0.192674\ttrain-rmspe:0.198805\n",
      "[56]\teval-rmspe:0.192160\ttrain-rmspe:0.197933\n",
      "[57]\teval-rmspe:0.191880\ttrain-rmspe:0.197495\n",
      "[58]\teval-rmspe:0.191666\ttrain-rmspe:0.197342\n",
      "[59]\teval-rmspe:0.190709\ttrain-rmspe:0.196442\n",
      "[60]\teval-rmspe:0.190213\ttrain-rmspe:0.195919\n",
      "[61]\teval-rmspe:0.189101\ttrain-rmspe:0.192238\n",
      "[62]\teval-rmspe:0.187923\ttrain-rmspe:0.190640\n",
      "[63]\teval-rmspe:0.187321\ttrain-rmspe:0.190211\n",
      "[64]\teval-rmspe:0.187387\ttrain-rmspe:0.190113\n",
      "[65]\teval-rmspe:0.187235\ttrain-rmspe:0.189890\n",
      "[66]\teval-rmspe:0.186499\ttrain-rmspe:0.189419\n",
      "[67]\teval-rmspe:0.185208\ttrain-rmspe:0.188168\n",
      "[68]\teval-rmspe:0.185008\ttrain-rmspe:0.187848\n",
      "[69]\teval-rmspe:0.184077\ttrain-rmspe:0.187071\n",
      "[70]\teval-rmspe:0.183657\ttrain-rmspe:0.186572\n",
      "[71]\teval-rmspe:0.183375\ttrain-rmspe:0.186060\n",
      "[72]\teval-rmspe:0.182829\ttrain-rmspe:0.185427\n",
      "[73]\teval-rmspe:0.182084\ttrain-rmspe:0.184836\n",
      "[74]\teval-rmspe:0.181006\ttrain-rmspe:0.183985\n",
      "[75]\teval-rmspe:0.179543\ttrain-rmspe:0.183201\n",
      "[76]\teval-rmspe:0.179323\ttrain-rmspe:0.182924\n",
      "[77]\teval-rmspe:0.178834\ttrain-rmspe:0.182471\n",
      "[78]\teval-rmspe:0.178276\ttrain-rmspe:0.181747\n",
      "[79]\teval-rmspe:0.177640\ttrain-rmspe:0.181776\n",
      "[80]\teval-rmspe:0.176969\ttrain-rmspe:0.181265\n",
      "[81]\teval-rmspe:0.176444\ttrain-rmspe:0.181012\n",
      "[82]\teval-rmspe:0.176056\ttrain-rmspe:0.180204\n",
      "[83]\teval-rmspe:0.176076\ttrain-rmspe:0.180057\n",
      "[84]\teval-rmspe:0.175052\ttrain-rmspe:0.179282\n",
      "[85]\teval-rmspe:0.174501\ttrain-rmspe:0.178874\n",
      "[86]\teval-rmspe:0.173873\ttrain-rmspe:0.178352\n",
      "[87]\teval-rmspe:0.173312\ttrain-rmspe:0.178148\n",
      "[88]\teval-rmspe:0.173136\ttrain-rmspe:0.177805\n",
      "[89]\teval-rmspe:0.172634\ttrain-rmspe:0.177042\n",
      "[90]\teval-rmspe:0.172249\ttrain-rmspe:0.176618\n",
      "[91]\teval-rmspe:0.172222\ttrain-rmspe:0.176603\n",
      "[92]\teval-rmspe:0.171292\ttrain-rmspe:0.174952\n",
      "[93]\teval-rmspe:0.171194\ttrain-rmspe:0.174907\n",
      "[94]\teval-rmspe:0.170560\ttrain-rmspe:0.174305\n",
      "[95]\teval-rmspe:0.170487\ttrain-rmspe:0.174197\n",
      "[96]\teval-rmspe:0.169605\ttrain-rmspe:0.174324\n",
      "[97]\teval-rmspe:0.169029\ttrain-rmspe:0.173995\n",
      "[98]\teval-rmspe:0.168892\ttrain-rmspe:0.173857\n",
      "[99]\teval-rmspe:0.168669\ttrain-rmspe:0.173617\n",
      "[100]\teval-rmspe:0.168445\ttrain-rmspe:0.173413\n",
      "[101]\teval-rmspe:0.168165\ttrain-rmspe:0.172929\n",
      "[102]\teval-rmspe:0.167312\ttrain-rmspe:0.172592\n",
      "[103]\teval-rmspe:0.167032\ttrain-rmspe:0.172156\n",
      "[104]\teval-rmspe:0.166834\ttrain-rmspe:0.171850\n",
      "[105]\teval-rmspe:0.166505\ttrain-rmspe:0.171557\n",
      "[106]\teval-rmspe:0.166225\ttrain-rmspe:0.171234\n",
      "[107]\teval-rmspe:0.166121\ttrain-rmspe:0.170991\n",
      "[108]\teval-rmspe:0.165819\ttrain-rmspe:0.170988\n",
      "[109]\teval-rmspe:0.165541\ttrain-rmspe:0.171927\n",
      "[110]\teval-rmspe:0.165316\ttrain-rmspe:0.171856\n",
      "[111]\teval-rmspe:0.165118\ttrain-rmspe:0.171408\n",
      "[112]\teval-rmspe:0.164698\ttrain-rmspe:0.171138\n",
      "[113]\teval-rmspe:0.164465\ttrain-rmspe:0.170819\n",
      "[114]\teval-rmspe:0.164296\ttrain-rmspe:0.170750\n",
      "[115]\teval-rmspe:0.164161\ttrain-rmspe:0.170575\n",
      "[116]\teval-rmspe:0.164475\ttrain-rmspe:0.170215\n",
      "[117]\teval-rmspe:0.164316\ttrain-rmspe:0.169501\n",
      "[118]\teval-rmspe:0.163800\ttrain-rmspe:0.169254\n",
      "[119]\teval-rmspe:0.163742\ttrain-rmspe:0.169198\n",
      "[120]\teval-rmspe:0.163375\ttrain-rmspe:0.168921\n",
      "[121]\teval-rmspe:0.163108\ttrain-rmspe:0.168675\n",
      "[122]\teval-rmspe:0.163033\ttrain-rmspe:0.168487\n",
      "[123]\teval-rmspe:0.162948\ttrain-rmspe:0.168334\n",
      "[124]\teval-rmspe:0.162800\ttrain-rmspe:0.168168\n",
      "[125]\teval-rmspe:0.162694\ttrain-rmspe:0.167701\n",
      "[126]\teval-rmspe:0.162419\ttrain-rmspe:0.167453\n",
      "[127]\teval-rmspe:0.162263\ttrain-rmspe:0.167259\n",
      "[128]\teval-rmspe:0.162179\ttrain-rmspe:0.167141\n",
      "[129]\teval-rmspe:0.161310\ttrain-rmspe:0.169102\n",
      "[130]\teval-rmspe:0.161143\ttrain-rmspe:0.168551\n",
      "[131]\teval-rmspe:0.161104\ttrain-rmspe:0.168278\n",
      "[132]\teval-rmspe:0.160809\ttrain-rmspe:0.168064\n",
      "[133]\teval-rmspe:0.160474\ttrain-rmspe:0.167674\n",
      "[134]\teval-rmspe:0.160344\ttrain-rmspe:0.167532\n",
      "[135]\teval-rmspe:0.160223\ttrain-rmspe:0.167440\n",
      "[136]\teval-rmspe:0.160051\ttrain-rmspe:0.167232\n",
      "[137]\teval-rmspe:0.159857\ttrain-rmspe:0.167094\n",
      "[138]\teval-rmspe:0.159717\ttrain-rmspe:0.167040\n",
      "[139]\teval-rmspe:0.159567\ttrain-rmspe:0.166892\n",
      "[140]\teval-rmspe:0.159435\ttrain-rmspe:0.166727\n",
      "[141]\teval-rmspe:0.159801\ttrain-rmspe:0.166566\n",
      "[142]\teval-rmspe:0.159616\ttrain-rmspe:0.166421\n",
      "[143]\teval-rmspe:0.159394\ttrain-rmspe:0.166679\n",
      "[144]\teval-rmspe:0.159368\ttrain-rmspe:0.166585\n",
      "[145]\teval-rmspe:0.159064\ttrain-rmspe:0.166494\n",
      "[146]\teval-rmspe:0.159012\ttrain-rmspe:0.166467\n",
      "[147]\teval-rmspe:0.158988\ttrain-rmspe:0.166401\n",
      "[148]\teval-rmspe:0.158095\ttrain-rmspe:0.166124\n",
      "[149]\teval-rmspe:0.158002\ttrain-rmspe:0.166024\n",
      "[150]\teval-rmspe:0.157944\ttrain-rmspe:0.165890\n",
      "[151]\teval-rmspe:0.157685\ttrain-rmspe:0.165951\n",
      "[152]\teval-rmspe:0.157650\ttrain-rmspe:0.165838\n",
      "[153]\teval-rmspe:0.157510\ttrain-rmspe:0.165744\n",
      "[154]\teval-rmspe:0.157377\ttrain-rmspe:0.165621\n",
      "[155]\teval-rmspe:0.157248\ttrain-rmspe:0.165514\n",
      "[156]\teval-rmspe:0.157106\ttrain-rmspe:0.165442\n",
      "[157]\teval-rmspe:0.157029\ttrain-rmspe:0.165213\n",
      "[158]\teval-rmspe:0.156915\ttrain-rmspe:0.165510\n",
      "[159]\teval-rmspe:0.156807\ttrain-rmspe:0.165375\n",
      "[160]\teval-rmspe:0.156781\ttrain-rmspe:0.155221\n",
      "[161]\teval-rmspe:0.156561\ttrain-rmspe:0.154901\n",
      "[162]\teval-rmspe:0.156427\ttrain-rmspe:0.154713\n",
      "[163]\teval-rmspe:0.156008\ttrain-rmspe:0.154567\n",
      "[164]\teval-rmspe:0.155877\ttrain-rmspe:0.154268\n",
      "[165]\teval-rmspe:0.155841\ttrain-rmspe:0.154147\n",
      "[166]\teval-rmspe:0.155402\ttrain-rmspe:0.154003\n",
      "[167]\teval-rmspe:0.155530\ttrain-rmspe:0.153764\n",
      "[168]\teval-rmspe:0.155325\ttrain-rmspe:0.153776\n",
      "[169]\teval-rmspe:0.155241\ttrain-rmspe:0.153646\n",
      "[170]\teval-rmspe:0.155220\ttrain-rmspe:0.153577\n",
      "[171]\teval-rmspe:0.155154\ttrain-rmspe:0.153560\n",
      "[172]\teval-rmspe:0.155025\ttrain-rmspe:0.153448\n",
      "[173]\teval-rmspe:0.154927\ttrain-rmspe:0.153309\n",
      "[174]\teval-rmspe:0.154534\ttrain-rmspe:0.153106\n",
      "[175]\teval-rmspe:0.154521\ttrain-rmspe:0.152817\n",
      "[176]\teval-rmspe:0.154417\ttrain-rmspe:0.152746\n",
      "[177]\teval-rmspe:0.154386\ttrain-rmspe:0.152673\n",
      "[178]\teval-rmspe:0.154050\ttrain-rmspe:0.152531\n",
      "[179]\teval-rmspe:0.153981\ttrain-rmspe:0.152474\n",
      "[180]\teval-rmspe:0.153793\ttrain-rmspe:0.152268\n",
      "[181]\teval-rmspe:0.153781\ttrain-rmspe:0.152196\n",
      "[182]\teval-rmspe:0.153714\ttrain-rmspe:0.152064\n",
      "[183]\teval-rmspe:0.153829\ttrain-rmspe:0.151921\n",
      "[184]\teval-rmspe:0.153630\ttrain-rmspe:0.151827\n",
      "[185]\teval-rmspe:0.153360\ttrain-rmspe:0.151588\n",
      "[186]\teval-rmspe:0.153246\ttrain-rmspe:0.151480\n",
      "[187]\teval-rmspe:0.153200\ttrain-rmspe:0.151164\n",
      "[188]\teval-rmspe:0.153140\ttrain-rmspe:0.151082\n",
      "[189]\teval-rmspe:0.153050\ttrain-rmspe:0.150994\n",
      "[190]\teval-rmspe:0.152984\ttrain-rmspe:0.150930\n",
      "[191]\teval-rmspe:0.153033\ttrain-rmspe:0.150860\n",
      "[192]\teval-rmspe:0.152832\ttrain-rmspe:0.151555\n",
      "[193]\teval-rmspe:0.152686\ttrain-rmspe:0.151316\n",
      "[194]\teval-rmspe:0.152660\ttrain-rmspe:0.151285\n",
      "[195]\teval-rmspe:0.152467\ttrain-rmspe:0.151180\n",
      "[196]\teval-rmspe:0.152459\ttrain-rmspe:0.151149\n",
      "[197]\teval-rmspe:0.152365\ttrain-rmspe:0.151052\n",
      "[198]\teval-rmspe:0.152354\ttrain-rmspe:0.151233\n",
      "[199]\teval-rmspe:0.152194\ttrain-rmspe:0.151057\n",
      "[200]\teval-rmspe:0.152111\ttrain-rmspe:0.150854\n",
      "[201]\teval-rmspe:0.152074\ttrain-rmspe:0.150811\n",
      "[202]\teval-rmspe:0.152022\ttrain-rmspe:0.150733\n",
      "[203]\teval-rmspe:0.150881\ttrain-rmspe:0.150322\n",
      "[204]\teval-rmspe:0.150826\ttrain-rmspe:0.150273\n",
      "[205]\teval-rmspe:0.150549\ttrain-rmspe:0.150105\n",
      "[206]\teval-rmspe:0.150488\ttrain-rmspe:0.150034\n",
      "[207]\teval-rmspe:0.150413\ttrain-rmspe:0.149933\n",
      "[208]\teval-rmspe:0.150336\ttrain-rmspe:0.149864\n",
      "[209]\teval-rmspe:0.149954\ttrain-rmspe:0.149835\n",
      "[210]\teval-rmspe:0.149853\ttrain-rmspe:0.150874\n",
      "[211]\teval-rmspe:0.149871\ttrain-rmspe:0.150835\n",
      "[212]\teval-rmspe:0.149878\ttrain-rmspe:0.150754\n",
      "[213]\teval-rmspe:0.149728\ttrain-rmspe:0.150848\n",
      "[214]\teval-rmspe:0.149622\ttrain-rmspe:0.150765\n",
      "[215]\teval-rmspe:0.149459\ttrain-rmspe:0.150661\n",
      "[216]\teval-rmspe:0.148934\ttrain-rmspe:0.150578\n",
      "[217]\teval-rmspe:0.148868\ttrain-rmspe:0.150578\n",
      "[218]\teval-rmspe:0.148866\ttrain-rmspe:0.150453\n",
      "[219]\teval-rmspe:0.148761\ttrain-rmspe:0.150380\n",
      "[220]\teval-rmspe:0.148901\ttrain-rmspe:0.149935\n",
      "[221]\teval-rmspe:0.148881\ttrain-rmspe:0.149886\n",
      "[222]\teval-rmspe:0.148877\ttrain-rmspe:0.149894\n",
      "[223]\teval-rmspe:0.148830\ttrain-rmspe:0.149821\n",
      "[224]\teval-rmspe:0.148727\ttrain-rmspe:0.149710\n",
      "[225]\teval-rmspe:0.148683\ttrain-rmspe:0.149574\n",
      "[226]\teval-rmspe:0.148652\ttrain-rmspe:0.149502\n",
      "[227]\teval-rmspe:0.148587\ttrain-rmspe:0.149463\n",
      "[228]\teval-rmspe:0.148530\ttrain-rmspe:0.149370\n",
      "[229]\teval-rmspe:0.148444\ttrain-rmspe:0.149276\n",
      "[230]\teval-rmspe:0.148345\ttrain-rmspe:0.149182\n",
      "[231]\teval-rmspe:0.148324\ttrain-rmspe:0.148592\n",
      "[232]\teval-rmspe:0.148231\ttrain-rmspe:0.148563\n",
      "[233]\teval-rmspe:0.147629\ttrain-rmspe:0.148389\n",
      "[234]\teval-rmspe:0.147475\ttrain-rmspe:0.145230\n",
      "[235]\teval-rmspe:0.147215\ttrain-rmspe:0.145159\n",
      "[236]\teval-rmspe:0.147169\ttrain-rmspe:0.145097\n",
      "[237]\teval-rmspe:0.147044\ttrain-rmspe:0.145020\n",
      "[238]\teval-rmspe:0.147037\ttrain-rmspe:0.144924\n",
      "[239]\teval-rmspe:0.146994\ttrain-rmspe:0.144886\n",
      "[240]\teval-rmspe:0.147040\ttrain-rmspe:0.144787\n",
      "[241]\teval-rmspe:0.146907\ttrain-rmspe:0.144683\n",
      "[242]\teval-rmspe:0.146734\ttrain-rmspe:0.144641\n",
      "[243]\teval-rmspe:0.146729\ttrain-rmspe:0.144604\n",
      "[244]\teval-rmspe:0.146712\ttrain-rmspe:0.144576\n",
      "[245]\teval-rmspe:0.146747\ttrain-rmspe:0.144567\n",
      "[246]\teval-rmspe:0.146711\ttrain-rmspe:0.144525\n",
      "[247]\teval-rmspe:0.146529\ttrain-rmspe:0.130872\n",
      "[248]\teval-rmspe:0.146484\ttrain-rmspe:0.130831\n",
      "[249]\teval-rmspe:0.146415\ttrain-rmspe:0.130748\n",
      "[250]\teval-rmspe:0.146416\ttrain-rmspe:0.130706\n",
      "[251]\teval-rmspe:0.146332\ttrain-rmspe:0.130557\n",
      "[252]\teval-rmspe:0.146271\ttrain-rmspe:0.130351\n",
      "[253]\teval-rmspe:0.146182\ttrain-rmspe:0.130305\n",
      "[254]\teval-rmspe:0.146081\ttrain-rmspe:0.130181\n",
      "[255]\teval-rmspe:0.145991\ttrain-rmspe:0.130051\n",
      "[256]\teval-rmspe:0.145937\ttrain-rmspe:0.129812\n",
      "[257]\teval-rmspe:0.145825\ttrain-rmspe:0.129732\n",
      "[258]\teval-rmspe:0.145761\ttrain-rmspe:0.129699\n",
      "[259]\teval-rmspe:0.145548\ttrain-rmspe:0.129636\n",
      "[260]\teval-rmspe:0.145451\ttrain-rmspe:0.129299\n",
      "[261]\teval-rmspe:0.145427\ttrain-rmspe:0.129254\n",
      "[262]\teval-rmspe:0.145435\ttrain-rmspe:0.129183\n",
      "[263]\teval-rmspe:0.145499\ttrain-rmspe:0.129213\n",
      "[264]\teval-rmspe:0.145297\ttrain-rmspe:0.129054\n",
      "[265]\teval-rmspe:0.145327\ttrain-rmspe:0.128742\n",
      "[266]\teval-rmspe:0.145308\ttrain-rmspe:0.128718\n",
      "[267]\teval-rmspe:0.145307\ttrain-rmspe:0.128673\n",
      "[268]\teval-rmspe:0.145510\ttrain-rmspe:0.128603\n",
      "[269]\teval-rmspe:0.145336\ttrain-rmspe:0.128559\n",
      "[270]\teval-rmspe:0.145303\ttrain-rmspe:0.128523\n",
      "[271]\teval-rmspe:0.145233\ttrain-rmspe:0.128498\n",
      "[272]\teval-rmspe:0.145456\ttrain-rmspe:0.128365\n",
      "[273]\teval-rmspe:0.145482\ttrain-rmspe:0.128332\n",
      "[274]\teval-rmspe:0.145473\ttrain-rmspe:0.128324\n",
      "[275]\teval-rmspe:0.145262\ttrain-rmspe:0.128256\n",
      "[276]\teval-rmspe:0.145202\ttrain-rmspe:0.128164\n",
      "[277]\teval-rmspe:0.145175\ttrain-rmspe:0.127934\n",
      "[278]\teval-rmspe:0.145166\ttrain-rmspe:0.127926\n",
      "[279]\teval-rmspe:0.144848\ttrain-rmspe:0.127858\n",
      "[280]\teval-rmspe:0.144485\ttrain-rmspe:0.127778\n",
      "[281]\teval-rmspe:0.144465\ttrain-rmspe:0.127710\n",
      "[282]\teval-rmspe:0.144412\ttrain-rmspe:0.127656\n",
      "[283]\teval-rmspe:0.144377\ttrain-rmspe:0.127624\n",
      "[284]\teval-rmspe:0.144377\ttrain-rmspe:0.127572\n",
      "[285]\teval-rmspe:0.144356\ttrain-rmspe:0.127547\n",
      "[286]\teval-rmspe:0.144322\ttrain-rmspe:0.127498\n",
      "[287]\teval-rmspe:0.144287\ttrain-rmspe:0.127481\n",
      "[288]\teval-rmspe:0.144249\ttrain-rmspe:0.127432\n",
      "[289]\teval-rmspe:0.141716\ttrain-rmspe:0.127167\n",
      "[290]\teval-rmspe:0.141656\ttrain-rmspe:0.127078\n",
      "[291]\teval-rmspe:0.141611\ttrain-rmspe:0.127063\n",
      "[292]\teval-rmspe:0.141532\ttrain-rmspe:0.127038\n",
      "[293]\teval-rmspe:0.141509\ttrain-rmspe:0.127009\n",
      "[294]\teval-rmspe:0.141484\ttrain-rmspe:0.127002\n",
      "[295]\teval-rmspe:0.141435\ttrain-rmspe:0.126929\n",
      "[296]\teval-rmspe:0.141150\ttrain-rmspe:0.124099\n",
      "[297]\teval-rmspe:0.141125\ttrain-rmspe:0.124052\n",
      "[298]\teval-rmspe:0.141244\ttrain-rmspe:0.124035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a XGBoost model\n",
      "844391    2013-01-02\n",
      "Name: Date, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[299]\teval-rmspe:0.141263\ttrain-rmspe:0.124010\n"
     ]
    }
   ],
   "source": [
    "print(\"Train a XGBoost model\")\n",
    "val_size = 100000\n",
    "#train = train.sort(['Date'])\n",
    "print(train.tail(1)['Date'])\n",
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "#X_train, X_test = train.head(len(train) - val_size), train.tail(val_size)\n",
    "dtrain = xgb.DMatrix(X_train[features], np.log(X_train[\"Sales\"] + 1))\n",
    "dvalid = xgb.DMatrix(X_test[features], np.log(X_test[\"Sales\"] + 1))\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, feval=rmspe_xg, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "error 0.141262991418\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating\")\n",
    "train_probs = gbm.predict(xgb.DMatrix(X_test[features]))\n",
    "indices = train_probs < 0\n",
    "train_probs[indices] = 0\n",
    "error = rmspe(np.exp(train_probs) - 1, X_test['Sales'].values)\n",
    "print('error', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions on the test set\n"
     ]
    }
   ],
   "source": [
    "print(\"Make predictions on the test set\")\n",
    "test_probs = gbm.predict(xgb.DMatrix(test[features]))\n",
    "indices = test_probs < 0\n",
    "test_probs[indices] = 0\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Sales\": np.exp(test_probs) - 1})\n",
    "submission.to_csv(\"xgboost_kscript_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
