{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") #Needed to save figures\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "\n",
    "# Gather some features\n",
    "def build_features(features, data):\n",
    "    # remove NaNs\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "    # Use some properties directly\n",
    "    features.extend(['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday'])\n",
    "\n",
    "    # Label encode some features\n",
    "    features.extend(['StoreType', 'Assortment', 'StateHoliday'])\n",
    "    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n",
    "    data.StoreType.replace(mappings, inplace=True)\n",
    "    data.Assortment.replace(mappings, inplace=True)\n",
    "    data.StateHoliday.replace(mappings, inplace=True)\n",
    "\n",
    "    features.extend(['DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear'])\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['DayOfWeek'] = data.Date.dt.dayofweek\n",
    "    data['WeekOfYear'] = data.Date.dt.weekofyear\n",
    "\n",
    "    # Calculate time competition open time in months\n",
    "    features.append('CompetitionOpen')\n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n",
    "        (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    # Promo open time in months\n",
    "    features.append('PromoOpen')\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n",
    "        (data.WeekOfYear - data.Promo2SinceWeek) / 4.0\n",
    "    data['PromoOpen'] = data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "    data.loc[data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "\n",
    "    # Indicate that sales on that day are in promo interval\n",
    "    features.append('IsPromoMonth')\n",
    "    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n",
    "             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "    data['monthStr'] = data.Month.map(month2str)\n",
    "    data.loc[data.PromoInterval == 0, 'PromoInterval'] = ''\n",
    "    data['IsPromoMonth'] = 0\n",
    "    for interval in data.PromoInterval.unique():\n",
    "        if interval != '':\n",
    "            for month in interval.split(','):\n",
    "                data.loc[(data.monthStr == month) & (data.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the training, test and store data using pandas\n"
     ]
    }
   ],
   "source": [
    "## Start of main script\n",
    "\n",
    "print(\"Load the training, test and store data using pandas\")\n",
    "types = {'CompetitionOpenSinceYear': np.dtype(int),\n",
    "         'CompetitionOpenSinceMonth': np.dtype(int),\n",
    "         'StateHoliday': np.dtype(str),\n",
    "         'Promo2SinceWeek': np.dtype(int),\n",
    "         'SchoolHoliday': np.dtype(float),\n",
    "         'PromoInterval': np.dtype(str)}\n",
    "train = pd.read_csv(\"train.csv\", parse_dates=[2], dtype=types)\n",
    "test = pd.read_csv(\"test.csv\", parse_dates=[3], dtype=types)\n",
    "store = pd.read_csv(\"store.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume store open, if not provided\n"
     ]
    }
   ],
   "source": [
    "print(\"Assume store open, if not provided\")\n",
    "train.fillna(1, inplace=True)\n",
    "test.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider only open stores for training. Closed stores wont count into the score.\n",
      "Use only Sales bigger then zero. Simplifies calculation of rmspe\n"
     ]
    }
   ],
   "source": [
    "print(\"Consider only open stores for training. Closed stores wont count into the score.\")\n",
    "train = train[train[\"Open\"] != 0]\n",
    "print(\"Use only Sales bigger then zero. Simplifies calculation of rmspe\")\n",
    "train = train[train[\"Sales\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join with store\n"
     ]
    }
   ],
   "source": [
    "print(\"Join with store\")\n",
    "train = pd.merge(train, store, on='Store')\n",
    "test = pd.merge(test, store, on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment features\n",
      "['Store', 'CompetitionDistance', 'Promo', 'Promo2', 'SchoolHoliday', 'StoreType', 'Assortment', 'StateHoliday', 'DayOfWeek', 'Month', 'Day', 'Year', 'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "\n",
    "print(\"augment features\")\n",
    "build_features(features, train)\n",
    "build_features([], test)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data processed\n"
     ]
    }
   ],
   "source": [
    "print('training data processed')\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\" : \"gbtree\",\n",
    "          \"eta\": 0.3,\n",
    "          \"max_depth\": 10,\n",
    "          \"subsample\": 0.9,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1,\n",
    "          \"seed\": 1301\n",
    "          }\n",
    "num_boost_round = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train a XGBoost model\n"
     ]
    }
   ],
   "source": [
    "print(\"Train a XGBoost model\")\n",
    "X_train, X_valid = train_test_split(train, test_size=0.012, random_state=10)\n",
    "y_train = np.log1p(X_train.Sales)\n",
    "y_valid = np.log1p(X_valid.Sales)\n",
    "dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "dvalid = xgb.DMatrix(X_valid[features], y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-rmspe:0.996844\teval-rmspe:0.996842\n",
      "[1]\ttrain-rmspe:0.981529\teval-rmspe:0.981516\n",
      "[2]\ttrain-rmspe:0.937905\teval-rmspe:0.937862\n",
      "[3]\ttrain-rmspe:0.856194\teval-rmspe:0.856098\n",
      "[4]\ttrain-rmspe:0.743160\teval-rmspe:0.742726\n",
      "[5]\ttrain-rmspe:0.618875\teval-rmspe:0.617223\n",
      "[6]\ttrain-rmspe:0.503698\teval-rmspe:0.499682\n",
      "[7]\ttrain-rmspe:0.412827\teval-rmspe:0.405096\n",
      "[8]\ttrain-rmspe:0.348803\teval-rmspe:0.336769\n",
      "[9]\ttrain-rmspe:0.314328\teval-rmspe:0.297099\n",
      "[10]\ttrain-rmspe:0.299580\teval-rmspe:0.278183\n",
      "[11]\ttrain-rmspe:0.290363\teval-rmspe:0.265933\n",
      "[12]\ttrain-rmspe:0.286387\teval-rmspe:0.259238\n",
      "[13]\ttrain-rmspe:0.286738\teval-rmspe:0.257671\n",
      "[14]\ttrain-rmspe:0.272153\teval-rmspe:0.241290\n",
      "[15]\ttrain-rmspe:0.272696\teval-rmspe:0.242140\n",
      "[16]\ttrain-rmspe:0.272499\teval-rmspe:0.241842\n",
      "[17]\ttrain-rmspe:0.268873\teval-rmspe:0.237345\n",
      "[18]\ttrain-rmspe:0.268490\teval-rmspe:0.236968\n",
      "[19]\ttrain-rmspe:0.265031\teval-rmspe:0.230337\n",
      "[20]\ttrain-rmspe:0.263467\teval-rmspe:0.227234\n",
      "[21]\ttrain-rmspe:0.259947\teval-rmspe:0.224255\n",
      "[22]\ttrain-rmspe:0.253835\teval-rmspe:0.217527\n",
      "[23]\ttrain-rmspe:0.245131\teval-rmspe:0.208001\n",
      "[24]\ttrain-rmspe:0.236018\teval-rmspe:0.197779\n",
      "[25]\ttrain-rmspe:0.233812\teval-rmspe:0.194616\n",
      "[26]\ttrain-rmspe:0.228386\teval-rmspe:0.187861\n",
      "[27]\ttrain-rmspe:0.227147\teval-rmspe:0.186746\n",
      "[28]\ttrain-rmspe:0.223817\teval-rmspe:0.181753\n",
      "[29]\ttrain-rmspe:0.222877\teval-rmspe:0.180291\n",
      "[30]\ttrain-rmspe:0.217230\teval-rmspe:0.174015\n",
      "[31]\ttrain-rmspe:0.211997\teval-rmspe:0.172656\n",
      "[32]\ttrain-rmspe:0.209899\teval-rmspe:0.169601\n",
      "[33]\ttrain-rmspe:0.208163\teval-rmspe:0.167819\n",
      "[34]\ttrain-rmspe:0.206537\teval-rmspe:0.166128\n",
      "[35]\ttrain-rmspe:0.205523\teval-rmspe:0.165199\n",
      "[36]\ttrain-rmspe:0.205110\teval-rmspe:0.164673\n",
      "[37]\ttrain-rmspe:0.202233\teval-rmspe:0.161219\n",
      "[38]\ttrain-rmspe:0.200833\teval-rmspe:0.159706\n",
      "[39]\ttrain-rmspe:0.198873\teval-rmspe:0.157291\n",
      "[40]\ttrain-rmspe:0.197503\teval-rmspe:0.155848\n",
      "[41]\ttrain-rmspe:0.195905\teval-rmspe:0.153967\n",
      "[42]\ttrain-rmspe:0.196416\teval-rmspe:0.153037\n",
      "[43]\ttrain-rmspe:0.194890\teval-rmspe:0.151859\n",
      "[44]\ttrain-rmspe:0.188518\teval-rmspe:0.151512\n",
      "[45]\ttrain-rmspe:0.187501\teval-rmspe:0.150564\n",
      "[46]\ttrain-rmspe:0.185118\teval-rmspe:0.147258\n",
      "[47]\ttrain-rmspe:0.183824\teval-rmspe:0.146036\n",
      "[48]\ttrain-rmspe:0.181403\teval-rmspe:0.143168\n",
      "[49]\ttrain-rmspe:0.180590\teval-rmspe:0.142237\n",
      "[50]\ttrain-rmspe:0.179971\teval-rmspe:0.141303\n",
      "[51]\ttrain-rmspe:0.178792\teval-rmspe:0.139787\n",
      "[52]\ttrain-rmspe:0.177903\teval-rmspe:0.138626\n",
      "[53]\ttrain-rmspe:0.176672\teval-rmspe:0.137825\n",
      "[54]\ttrain-rmspe:0.176210\teval-rmspe:0.137319\n",
      "[55]\ttrain-rmspe:0.174909\teval-rmspe:0.135825\n",
      "[56]\ttrain-rmspe:0.174306\teval-rmspe:0.134776\n",
      "[57]\ttrain-rmspe:0.173272\teval-rmspe:0.133805\n",
      "[58]\ttrain-rmspe:0.172817\teval-rmspe:0.133413\n",
      "[59]\ttrain-rmspe:0.171757\teval-rmspe:0.131733\n",
      "[60]\ttrain-rmspe:0.170665\teval-rmspe:0.130354\n",
      "[61]\ttrain-rmspe:0.171936\teval-rmspe:0.129272\n",
      "[62]\ttrain-rmspe:0.171619\teval-rmspe:0.128971\n",
      "[63]\ttrain-rmspe:0.170904\teval-rmspe:0.128120\n",
      "[64]\ttrain-rmspe:0.170585\teval-rmspe:0.127594\n",
      "[65]\ttrain-rmspe:0.170240\teval-rmspe:0.127432\n",
      "[66]\ttrain-rmspe:0.169697\teval-rmspe:0.127082\n",
      "[67]\ttrain-rmspe:0.166476\teval-rmspe:0.126636\n",
      "[68]\ttrain-rmspe:0.165646\teval-rmspe:0.125737\n",
      "[69]\ttrain-rmspe:0.145368\teval-rmspe:0.125042\n",
      "[70]\ttrain-rmspe:0.145290\teval-rmspe:0.124948\n",
      "[71]\ttrain-rmspe:0.144973\teval-rmspe:0.124681\n",
      "[72]\ttrain-rmspe:0.144691\teval-rmspe:0.123749\n",
      "[73]\ttrain-rmspe:0.144289\teval-rmspe:0.123385\n",
      "[74]\ttrain-rmspe:0.143977\teval-rmspe:0.123081\n",
      "[75]\ttrain-rmspe:0.143442\teval-rmspe:0.122451\n",
      "[76]\ttrain-rmspe:0.143189\teval-rmspe:0.122150\n",
      "[77]\ttrain-rmspe:0.142732\teval-rmspe:0.121572\n",
      "[78]\ttrain-rmspe:0.142676\teval-rmspe:0.121498\n",
      "[79]\ttrain-rmspe:0.142154\teval-rmspe:0.120692\n",
      "[80]\ttrain-rmspe:0.141716\teval-rmspe:0.120112\n",
      "[81]\ttrain-rmspe:0.141275\teval-rmspe:0.119804\n",
      "[82]\ttrain-rmspe:0.141031\teval-rmspe:0.119537\n",
      "[83]\ttrain-rmspe:0.140461\teval-rmspe:0.118976\n",
      "[84]\ttrain-rmspe:0.140020\teval-rmspe:0.118640\n",
      "[85]\ttrain-rmspe:0.138968\teval-rmspe:0.118017\n",
      "[86]\ttrain-rmspe:0.136439\teval-rmspe:0.117537\n",
      "[87]\ttrain-rmspe:0.136005\teval-rmspe:0.117156\n",
      "[88]\ttrain-rmspe:0.135497\teval-rmspe:0.117055\n",
      "[89]\ttrain-rmspe:0.134076\teval-rmspe:0.116734\n",
      "[90]\ttrain-rmspe:0.133556\teval-rmspe:0.116305\n",
      "[91]\ttrain-rmspe:0.133217\teval-rmspe:0.115984\n",
      "[92]\ttrain-rmspe:0.132919\teval-rmspe:0.115602\n",
      "[93]\ttrain-rmspe:0.132492\teval-rmspe:0.115252\n",
      "[94]\ttrain-rmspe:0.131960\teval-rmspe:0.114618\n",
      "[95]\ttrain-rmspe:0.131389\teval-rmspe:0.113939\n",
      "[96]\ttrain-rmspe:0.130911\teval-rmspe:0.113406\n",
      "[97]\ttrain-rmspe:0.130455\teval-rmspe:0.113017\n",
      "[98]\ttrain-rmspe:0.130221\teval-rmspe:0.112838\n",
      "[99]\ttrain-rmspe:0.130082\teval-rmspe:0.112718\n",
      "[100]\ttrain-rmspe:0.129303\teval-rmspe:0.112630\n",
      "[101]\ttrain-rmspe:0.128785\teval-rmspe:0.112294\n",
      "[102]\ttrain-rmspe:0.128470\teval-rmspe:0.111925\n",
      "[103]\ttrain-rmspe:0.128172\teval-rmspe:0.111617\n",
      "[104]\ttrain-rmspe:0.128016\teval-rmspe:0.111459\n",
      "[105]\ttrain-rmspe:0.127756\teval-rmspe:0.111218\n",
      "[106]\ttrain-rmspe:0.126414\teval-rmspe:0.111013\n",
      "[107]\ttrain-rmspe:0.125981\teval-rmspe:0.110809\n",
      "[108]\ttrain-rmspe:0.125756\teval-rmspe:0.110698\n",
      "[109]\ttrain-rmspe:0.125402\teval-rmspe:0.110485\n",
      "[110]\ttrain-rmspe:0.125116\teval-rmspe:0.110248\n",
      "[111]\ttrain-rmspe:0.124675\teval-rmspe:0.109895\n",
      "[112]\ttrain-rmspe:0.124500\teval-rmspe:0.109746\n",
      "[113]\ttrain-rmspe:0.124354\teval-rmspe:0.109635\n",
      "[114]\ttrain-rmspe:0.123863\teval-rmspe:0.109437\n",
      "[115]\ttrain-rmspe:0.123330\teval-rmspe:0.109385\n",
      "[116]\ttrain-rmspe:0.123169\teval-rmspe:0.109308\n",
      "[117]\ttrain-rmspe:0.122806\teval-rmspe:0.109079\n",
      "[118]\ttrain-rmspe:0.122600\teval-rmspe:0.108991\n",
      "[119]\ttrain-rmspe:0.122452\teval-rmspe:0.108787\n",
      "[120]\ttrain-rmspe:0.122338\teval-rmspe:0.108704\n",
      "[121]\ttrain-rmspe:0.122103\teval-rmspe:0.108539\n",
      "[122]\ttrain-rmspe:0.121972\teval-rmspe:0.108476\n",
      "[123]\ttrain-rmspe:0.121656\teval-rmspe:0.108281\n",
      "[124]\ttrain-rmspe:0.121206\teval-rmspe:0.107958\n",
      "[125]\ttrain-rmspe:0.120835\teval-rmspe:0.107631\n",
      "[126]\ttrain-rmspe:0.120710\teval-rmspe:0.107541\n",
      "[127]\ttrain-rmspe:0.120452\teval-rmspe:0.107326\n",
      "[128]\ttrain-rmspe:0.120312\teval-rmspe:0.107293\n",
      "[129]\ttrain-rmspe:0.120069\teval-rmspe:0.107174\n",
      "[130]\ttrain-rmspe:0.119698\teval-rmspe:0.107093\n",
      "[131]\ttrain-rmspe:0.119597\teval-rmspe:0.107019\n",
      "[132]\ttrain-rmspe:0.119461\teval-rmspe:0.106925\n",
      "[133]\ttrain-rmspe:0.119417\teval-rmspe:0.106889\n",
      "[134]\ttrain-rmspe:0.119084\teval-rmspe:0.106534\n",
      "[135]\ttrain-rmspe:0.112362\teval-rmspe:0.106373\n",
      "[136]\ttrain-rmspe:0.112153\teval-rmspe:0.106288\n",
      "[137]\ttrain-rmspe:0.111484\teval-rmspe:0.106140\n",
      "[138]\ttrain-rmspe:0.111178\teval-rmspe:0.106021\n",
      "[139]\ttrain-rmspe:0.110868\teval-rmspe:0.105928\n",
      "[140]\ttrain-rmspe:0.110673\teval-rmspe:0.105810\n",
      "[141]\ttrain-rmspe:0.110519\teval-rmspe:0.105703\n",
      "[142]\ttrain-rmspe:0.110313\teval-rmspe:0.105524\n",
      "[143]\ttrain-rmspe:0.110246\teval-rmspe:0.105496\n",
      "[144]\ttrain-rmspe:0.110113\teval-rmspe:0.105428\n",
      "[145]\ttrain-rmspe:0.110066\teval-rmspe:0.105386\n",
      "[146]\ttrain-rmspe:0.109738\teval-rmspe:0.105232\n",
      "[147]\ttrain-rmspe:0.109209\teval-rmspe:0.105114\n",
      "[148]\ttrain-rmspe:0.109038\teval-rmspe:0.105088\n",
      "[149]\ttrain-rmspe:0.108978\teval-rmspe:0.105062\n",
      "[150]\ttrain-rmspe:0.108879\teval-rmspe:0.105044\n",
      "[151]\ttrain-rmspe:0.108739\teval-rmspe:0.104974\n",
      "[152]\ttrain-rmspe:0.108622\teval-rmspe:0.104858\n",
      "[153]\ttrain-rmspe:0.108408\teval-rmspe:0.104714\n",
      "[154]\ttrain-rmspe:0.108301\teval-rmspe:0.104682\n",
      "[155]\ttrain-rmspe:0.108211\teval-rmspe:0.104672\n",
      "[156]\ttrain-rmspe:0.108038\teval-rmspe:0.104636\n",
      "[157]\ttrain-rmspe:0.107880\teval-rmspe:0.104622\n",
      "[158]\ttrain-rmspe:0.107724\teval-rmspe:0.104582\n",
      "[159]\ttrain-rmspe:0.107472\teval-rmspe:0.104417\n",
      "[160]\ttrain-rmspe:0.107242\teval-rmspe:0.104256\n",
      "[161]\ttrain-rmspe:0.107042\teval-rmspe:0.104150\n",
      "[162]\ttrain-rmspe:0.106920\teval-rmspe:0.104048\n",
      "[163]\ttrain-rmspe:0.106797\teval-rmspe:0.103966\n",
      "[164]\ttrain-rmspe:0.106462\teval-rmspe:0.103739\n",
      "[165]\ttrain-rmspe:0.106192\teval-rmspe:0.103681\n",
      "[166]\ttrain-rmspe:0.106048\teval-rmspe:0.103404\n",
      "[167]\ttrain-rmspe:0.105969\teval-rmspe:0.103371\n",
      "[168]\ttrain-rmspe:0.105926\teval-rmspe:0.103342\n",
      "[169]\ttrain-rmspe:0.105361\teval-rmspe:0.103284\n",
      "[170]\ttrain-rmspe:0.105231\teval-rmspe:0.103251\n",
      "[171]\ttrain-rmspe:0.102138\teval-rmspe:0.103208\n",
      "[172]\ttrain-rmspe:0.101965\teval-rmspe:0.103149\n",
      "[173]\ttrain-rmspe:0.101702\teval-rmspe:0.103058\n",
      "[174]\ttrain-rmspe:0.098932\teval-rmspe:0.102929\n",
      "[175]\ttrain-rmspe:0.098853\teval-rmspe:0.102923\n",
      "[176]\ttrain-rmspe:0.098754\teval-rmspe:0.102916\n",
      "[177]\ttrain-rmspe:0.098539\teval-rmspe:0.102799\n",
      "[178]\ttrain-rmspe:0.098134\teval-rmspe:0.102792\n",
      "[179]\ttrain-rmspe:0.098085\teval-rmspe:0.102812\n",
      "[180]\ttrain-rmspe:0.097940\teval-rmspe:0.102724\n",
      "[181]\ttrain-rmspe:0.097822\teval-rmspe:0.102717\n",
      "[182]\ttrain-rmspe:0.097604\teval-rmspe:0.102586\n",
      "[183]\ttrain-rmspe:0.097367\teval-rmspe:0.102560\n",
      "[184]\ttrain-rmspe:0.097228\teval-rmspe:0.102480\n",
      "[185]\ttrain-rmspe:0.097066\teval-rmspe:0.102079\n",
      "[186]\ttrain-rmspe:0.096967\teval-rmspe:0.102041\n",
      "[187]\ttrain-rmspe:0.096743\teval-rmspe:0.101918\n",
      "[188]\ttrain-rmspe:0.096623\teval-rmspe:0.101857\n",
      "[189]\ttrain-rmspe:0.095910\teval-rmspe:0.101835\n",
      "[190]\ttrain-rmspe:0.095747\teval-rmspe:0.101794\n",
      "[191]\ttrain-rmspe:0.095587\teval-rmspe:0.101746\n",
      "[192]\ttrain-rmspe:0.095370\teval-rmspe:0.101641\n",
      "[193]\ttrain-rmspe:0.095183\teval-rmspe:0.101595\n",
      "[194]\ttrain-rmspe:0.095121\teval-rmspe:0.101568\n",
      "[195]\ttrain-rmspe:0.094986\teval-rmspe:0.101507\n",
      "[196]\ttrain-rmspe:0.094899\teval-rmspe:0.101426\n",
      "[197]\ttrain-rmspe:0.094748\teval-rmspe:0.101388\n",
      "[198]\ttrain-rmspe:0.094526\teval-rmspe:0.101250\n",
      "[199]\ttrain-rmspe:0.094396\teval-rmspe:0.101203\n",
      "[200]\ttrain-rmspe:0.094288\teval-rmspe:0.101112\n",
      "[201]\ttrain-rmspe:0.094223\teval-rmspe:0.101083\n",
      "[202]\ttrain-rmspe:0.094060\teval-rmspe:0.100962\n",
      "[203]\ttrain-rmspe:0.093857\teval-rmspe:0.100938\n",
      "[204]\ttrain-rmspe:0.093743\teval-rmspe:0.100845\n",
      "[205]\ttrain-rmspe:0.093682\teval-rmspe:0.100818\n",
      "[206]\ttrain-rmspe:0.093510\teval-rmspe:0.100720\n",
      "[207]\ttrain-rmspe:0.093378\teval-rmspe:0.100637\n",
      "[208]\ttrain-rmspe:0.093304\teval-rmspe:0.100589\n",
      "[209]\ttrain-rmspe:0.093241\teval-rmspe:0.100598\n",
      "[210]\ttrain-rmspe:0.093086\teval-rmspe:0.100584\n",
      "[211]\ttrain-rmspe:0.092960\teval-rmspe:0.100557\n",
      "[212]\ttrain-rmspe:0.092808\teval-rmspe:0.100548\n",
      "[213]\ttrain-rmspe:0.092772\teval-rmspe:0.100511\n",
      "[214]\ttrain-rmspe:0.092568\teval-rmspe:0.100385\n",
      "[215]\ttrain-rmspe:0.092399\teval-rmspe:0.100374\n",
      "[216]\ttrain-rmspe:0.092259\teval-rmspe:0.100286\n",
      "[217]\ttrain-rmspe:0.092130\teval-rmspe:0.100222\n",
      "[218]\ttrain-rmspe:0.092021\teval-rmspe:0.100154\n",
      "[219]\ttrain-rmspe:0.091913\teval-rmspe:0.100077\n",
      "[220]\ttrain-rmspe:0.091837\teval-rmspe:0.100059\n",
      "[221]\ttrain-rmspe:0.091723\teval-rmspe:0.100048\n",
      "[222]\ttrain-rmspe:0.091633\teval-rmspe:0.099948\n",
      "[223]\ttrain-rmspe:0.091467\teval-rmspe:0.099942\n",
      "[224]\ttrain-rmspe:0.091417\teval-rmspe:0.099900\n",
      "[225]\ttrain-rmspe:0.091154\teval-rmspe:0.099879\n",
      "[226]\ttrain-rmspe:0.091001\teval-rmspe:0.099799\n",
      "[227]\ttrain-rmspe:0.090794\teval-rmspe:0.099590\n",
      "[228]\ttrain-rmspe:0.090749\teval-rmspe:0.099600\n",
      "[229]\ttrain-rmspe:0.090564\teval-rmspe:0.099463\n",
      "[230]\ttrain-rmspe:0.090447\teval-rmspe:0.099384\n",
      "[231]\ttrain-rmspe:0.090320\teval-rmspe:0.099322\n",
      "[232]\ttrain-rmspe:0.090276\teval-rmspe:0.099328\n",
      "[233]\ttrain-rmspe:0.090157\teval-rmspe:0.099275\n",
      "[234]\ttrain-rmspe:0.090062\teval-rmspe:0.099208\n",
      "[235]\ttrain-rmspe:0.090019\teval-rmspe:0.099210\n",
      "[236]\ttrain-rmspe:0.089912\teval-rmspe:0.099103\n",
      "[237]\ttrain-rmspe:0.089897\teval-rmspe:0.099089\n",
      "[238]\ttrain-rmspe:0.089791\teval-rmspe:0.099045\n",
      "[239]\ttrain-rmspe:0.089715\teval-rmspe:0.099021\n",
      "[240]\ttrain-rmspe:0.089633\teval-rmspe:0.098998\n",
      "[241]\ttrain-rmspe:0.089530\teval-rmspe:0.098979\n",
      "[242]\ttrain-rmspe:0.089436\teval-rmspe:0.098894\n",
      "[243]\ttrain-rmspe:0.089347\teval-rmspe:0.098865\n",
      "[244]\ttrain-rmspe:0.089227\teval-rmspe:0.098761\n",
      "[245]\ttrain-rmspe:0.089183\teval-rmspe:0.098742\n",
      "[246]\ttrain-rmspe:0.089133\teval-rmspe:0.098727\n",
      "[247]\ttrain-rmspe:0.089075\teval-rmspe:0.098726\n",
      "[248]\ttrain-rmspe:0.089008\teval-rmspe:0.098714\n",
      "[249]\ttrain-rmspe:0.088900\teval-rmspe:0.098598\n",
      "[250]\ttrain-rmspe:0.088206\teval-rmspe:0.098541\n",
      "[251]\ttrain-rmspe:0.088107\teval-rmspe:0.098496\n",
      "[252]\ttrain-rmspe:0.088067\teval-rmspe:0.098468\n",
      "[253]\ttrain-rmspe:0.088010\teval-rmspe:0.098445\n",
      "[254]\ttrain-rmspe:0.086872\teval-rmspe:0.098424\n",
      "[255]\ttrain-rmspe:0.086833\teval-rmspe:0.098361\n",
      "[256]\ttrain-rmspe:0.086712\teval-rmspe:0.098313\n",
      "[257]\ttrain-rmspe:0.086655\teval-rmspe:0.098323\n",
      "[258]\ttrain-rmspe:0.086430\teval-rmspe:0.098250\n",
      "[259]\ttrain-rmspe:0.086387\teval-rmspe:0.098237\n",
      "[260]\ttrain-rmspe:0.086292\teval-rmspe:0.098195\n",
      "[261]\ttrain-rmspe:0.086175\teval-rmspe:0.098114\n",
      "[262]\ttrain-rmspe:0.086048\teval-rmspe:0.098101\n",
      "[263]\ttrain-rmspe:0.085891\teval-rmspe:0.098065\n",
      "[264]\ttrain-rmspe:0.085797\teval-rmspe:0.098041\n",
      "[265]\ttrain-rmspe:0.085721\teval-rmspe:0.098018\n",
      "[266]\ttrain-rmspe:0.085408\teval-rmspe:0.098021\n",
      "[267]\ttrain-rmspe:0.085308\teval-rmspe:0.097995\n",
      "[268]\ttrain-rmspe:0.085189\teval-rmspe:0.097938\n",
      "[269]\ttrain-rmspe:0.085122\teval-rmspe:0.097929\n",
      "[270]\ttrain-rmspe:0.085094\teval-rmspe:0.097912\n",
      "[271]\ttrain-rmspe:0.085017\teval-rmspe:0.097876\n",
      "[272]\ttrain-rmspe:0.084918\teval-rmspe:0.097826\n",
      "[273]\ttrain-rmspe:0.084810\teval-rmspe:0.097772\n",
      "[274]\ttrain-rmspe:0.084722\teval-rmspe:0.097755\n",
      "[275]\ttrain-rmspe:0.084586\teval-rmspe:0.097617\n",
      "[276]\ttrain-rmspe:0.084514\teval-rmspe:0.097589\n",
      "[277]\ttrain-rmspe:0.084348\teval-rmspe:0.097564\n",
      "[278]\ttrain-rmspe:0.084273\teval-rmspe:0.097550\n",
      "[279]\ttrain-rmspe:0.084156\teval-rmspe:0.097497\n",
      "[280]\ttrain-rmspe:0.084049\teval-rmspe:0.097420\n",
      "[281]\ttrain-rmspe:0.083979\teval-rmspe:0.097425\n",
      "[282]\ttrain-rmspe:0.083887\teval-rmspe:0.097395\n",
      "[283]\ttrain-rmspe:0.083838\teval-rmspe:0.097354\n",
      "[284]\ttrain-rmspe:0.083819\teval-rmspe:0.097326\n",
      "[285]\ttrain-rmspe:0.083794\teval-rmspe:0.097297\n",
      "[286]\ttrain-rmspe:0.083701\teval-rmspe:0.097280\n",
      "[287]\ttrain-rmspe:0.083614\teval-rmspe:0.097264\n",
      "[288]\ttrain-rmspe:0.083519\teval-rmspe:0.097223\n",
      "[289]\ttrain-rmspe:0.083383\teval-rmspe:0.097210\n",
      "[290]\ttrain-rmspe:0.083346\teval-rmspe:0.097168\n",
      "[291]\ttrain-rmspe:0.083237\teval-rmspe:0.097137\n",
      "[292]\ttrain-rmspe:0.083160\teval-rmspe:0.097132\n",
      "[293]\ttrain-rmspe:0.083072\teval-rmspe:0.097073\n",
      "[294]\ttrain-rmspe:0.083042\teval-rmspe:0.097052\n",
      "[295]\ttrain-rmspe:0.082921\teval-rmspe:0.097055\n",
      "[296]\ttrain-rmspe:0.082898\teval-rmspe:0.097051\n",
      "[297]\ttrain-rmspe:0.082840\teval-rmspe:0.097052\n",
      "[298]\ttrain-rmspe:0.082767\teval-rmspe:0.096997\n",
      "[299]\ttrain-rmspe:0.082736\teval-rmspe:0.097012\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions on the test set\n"
     ]
    }
   ],
   "source": [
    "print(\"Make predictions on the test set\")\n",
    "dtest = xgb.DMatrix(test[features])\n",
    "test_probs = gbm.predict(dtest)\n",
    "# Make Submission\n",
    "result = pd.DataFrame({\"Id\": test[\"Id\"], 'Sales': np.expm1(test_probs)})\n",
    "result.to_csv(\"rossmann_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGB feature importances\n",
    "\n",
    "create_feature_map(features)\n",
    "importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "\n",
    "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "df['fscore'] = df['fscore'] / df['fscore'].sum()\n",
    "\n",
    "featp = df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "fig_featp = featp.get_figure()\n",
    "fig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
